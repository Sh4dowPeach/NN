{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_v2.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUNEL2l02VIN"
      },
      "source": [
        "!pip install -q tensorflow==2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8ijOj66-n77"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "import tensorflow.keras.models as M\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcSH4eMj2ZU3"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mbaR5Rk2bju"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywO7e8JL-n8A"
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_x = (train_x.reshape(-1, 28*28).astype(np.float32) - 127.5) / 127.5\n",
        "\n",
        "print(train_x.shape, train_x.dtype)\n",
        "print(np.min(train_x), np.max(train_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vidDx4IE-n8F"
      },
      "source": [
        "def plot_digits(samples):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    num = samples.shape[0]\n",
        "    for j in range(num):\n",
        "        ax = fig.add_subplot(8, 8, j+1)\n",
        "        ax.imshow(samples[j, ...].reshape(28, 28), cmap='gray')\n",
        "        plt.xticks([]), plt.yticks([])\n",
        "    plt.show()\n",
        "    \n",
        "plot_digits(train_x[:32, ...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGOgF51M-n8L"
      },
      "source": [
        "generator = M.Sequential([\n",
        "    L.Dense(256),\n",
        "    L.LeakyReLU(0.2),\n",
        "    L.BatchNormalization(momentum=0.8),\n",
        "    L.Dense(512),\n",
        "    L.LeakyReLU(0.2),\n",
        "    L.BatchNormalization(momentum=0.8),\n",
        "    L.Dense(1024),\n",
        "    L.LeakyReLU(0.2),\n",
        "    L.BatchNormalization(momentum=0.8),\n",
        "    L.Dense(784, activation='tanh'),\n",
        "])\n",
        "\n",
        "discriminator = M.Sequential([\n",
        "    L.Dense(784, activation=None),\n",
        "    L.LeakyReLU(alpha=0.2),\n",
        "    L.Dense(392, activation=None),\n",
        "    L.LeakyReLU(alpha=0.2),\n",
        "    L.Dense(1, activation=None),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueJAqwmM-n8O"
      },
      "source": [
        "INPUT_DIM = 100\n",
        "NUM_EPOCHS = 5\n",
        "HALF_BATCH_SIZE = 16\n",
        "BATCH_SIZE = HALF_BATCH_SIZE * 2\n",
        "LEARNING_RATE = 0.0002\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train_x)\n",
        "train_ds = train_ds.shuffle(buffer_size=train_x.shape[0])\n",
        "train_ds = train_ds.repeat(NUM_EPOCHS)\n",
        "train_ds = train_ds.batch(HALF_BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ym7JHIHA-n8R"
      },
      "source": [
        "for step, true_images in enumerate(train_ds):\n",
        "    \n",
        "    # Train Discriminator\n",
        "    \n",
        "    noise = np.random.normal(0, 1, (HALF_BATCH_SIZE, INPUT_DIM)).astype(np.float32)\n",
        "    syntetic_images = generator.predict(noise)\n",
        "    x_combined = np.concatenate((\n",
        "        true_images, \n",
        "        syntetic_images))\n",
        "    y_combined = np.concatenate((\n",
        "        np.ones((HALF_BATCH_SIZE, 1)), \n",
        "        np.zeros((HALF_BATCH_SIZE, 1))))\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = discriminator(x_combined)\n",
        "        d_loss_value = tf.compat.v1.losses.sigmoid_cross_entropy(y_combined, logits)\n",
        "    grads = tape.gradient(d_loss_value, discriminator.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
        "    \n",
        "    # Train Generator\n",
        "    \n",
        "    noise = np.random.normal(0, 1, (BATCH_SIZE, INPUT_DIM)).astype(np.float32)\n",
        "    y_mislabled = np.ones((BATCH_SIZE, 1))\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = discriminator(generator(noise, training=True))\n",
        "        g_loss_value = tf.compat.v1.losses.sigmoid_cross_entropy(y_mislabled, logits)\n",
        "    grads = tape.gradient(g_loss_value, generator.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
        "    \n",
        "    \n",
        "    \n",
        "    if step % 500 == 0:\n",
        "        print(\"[Step %2d] D Loss: %.4f; G Loss: %.4f\" % (\n",
        "            step, d_loss_value.numpy(), g_loss_value.numpy()))\n",
        "        noise = np.random.normal(0, 1, (8, INPUT_DIM)).astype(np.float32)\n",
        "        syntetic_images = generator.predict(noise)\n",
        "        plot_digits(syntetic_images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1l87EjA-n8Z"
      },
      "source": [
        "noise = np.random.normal(0, 1, (32, INPUT_DIM)).astype(np.float32)\n",
        "syntetic_images = generator.predict(noise)\n",
        "plot_digits(syntetic_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWhYJdXY-n8f"
      },
      "source": [
        "noise_1 = np.random.normal(0, 1, (INPUT_DIM)).astype(np.float32)\n",
        "noise_2 = np.random.normal(0, 1, (INPUT_DIM)).astype(np.float32)\n",
        "noise = np.linspace(noise_1, noise_2, 32)\n",
        "syntetic_images = generator.predict(noise)\n",
        "plot_digits(syntetic_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s6B1tEm-n8m"
      },
      "source": [
        "generator = M.Sequential([\n",
        "    L.Dense(128*7*7, activation=\"relu\"),\n",
        "    L.Reshape((7, 7, 128)),\n",
        "    L.UpSampling2D((2, 2)),    \n",
        "    L.Conv2D(128, (3, 3), padding=\"same\"),\n",
        "    L.BatchNormalization(momentum=0.8),\n",
        "    L.ReLU(),    \n",
        "    L.UpSampling2D((2, 2)),    \n",
        "    L.Conv2D(64, (3, 3), padding=\"same\"),\n",
        "    L.BatchNormalization(momentum=0.8),\n",
        "    L.ReLU(),    \n",
        "    L.Conv2D(1, (3, 3), padding=\"same\", activation='tanh'),\n",
        "])\n",
        "\n",
        "discriminator = M.Sequential([\n",
        "    L.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "    L.LeakyReLU(0.2),\n",
        "    L.Dropout(0.25),    \n",
        "    L.Conv2D(64, kernel_size=3, strides=(2, 2), padding=\"same\"),\n",
        "    L.ZeroPadding2D(padding=((0, 1), (0, 1))),\n",
        "    L.BatchNormalization(momentum=0.8),\n",
        "    L.LeakyReLU(alpha=0.2),\n",
        "    L.Dropout(0.25),\n",
        "    L.Conv2D(128, kernel_size=3, strides=(2, 2), padding=\"same\"),\n",
        "    L.BatchNormalization(momentum=0.8),\n",
        "    L.LeakyReLU(alpha=0.2),\n",
        "    L.Dropout(0.25),\n",
        "    L.Conv2D(256, kernel_size=3, strides=(1, 1), padding=\"same\"),\n",
        "    L.BatchNormalization(momentum=0.8),\n",
        "    L.LeakyReLU(alpha=0.2),\n",
        "    L.Dropout(0.25),\n",
        "    L.Flatten(),\n",
        "    L.Dense(1, activation=None),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb4EM7aY-n8q"
      },
      "source": [
        "INPUT_DIM = 100\n",
        "NUM_EPOCHS = 2\n",
        "HALF_BATCH_SIZE = 16\n",
        "BATCH_SIZE = HALF_BATCH_SIZE * 2\n",
        "LEARNING_RATE = 0.0002\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train_x.reshape(-1, 28, 28, 1))\n",
        "train_ds = train_ds.shuffle(buffer_size=train_x.shape[0])\n",
        "train_ds = train_ds.repeat(NUM_EPOCHS)\n",
        "train_ds = train_ds.batch(HALF_BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pGfWrqG4-n8t"
      },
      "source": [
        "for step, true_images in enumerate(train_ds):\n",
        "    \n",
        "    # Train Discriminator\n",
        "    \n",
        "    noise = np.random.normal(0, 1, (HALF_BATCH_SIZE, INPUT_DIM)).astype(np.float32)\n",
        "    syntetic_images = generator.predict(noise)\n",
        "    x_combined = np.concatenate((\n",
        "        true_images, \n",
        "        syntetic_images))\n",
        "    y_combined = np.concatenate((\n",
        "        np.ones((HALF_BATCH_SIZE, 1)), \n",
        "        np.zeros((HALF_BATCH_SIZE, 1))))\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = discriminator(x_combined, training=True)\n",
        "        d_loss_value = tf.compat.v1.losses.sigmoid_cross_entropy(y_combined, logits)\n",
        "    grads = tape.gradient(d_loss_value, discriminator.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
        "    \n",
        "    # Train Generator\n",
        "    \n",
        "    noise = np.random.normal(0, 1, (BATCH_SIZE, INPUT_DIM)).astype(np.float32)\n",
        "    y_mislabled = np.ones((BATCH_SIZE, 1))\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        syntetic = generator(noise, training=True)\n",
        "        logits = discriminator(syntetic, training=False)\n",
        "        g_loss_value = tf.compat.v1.losses.sigmoid_cross_entropy(y_mislabled, logits)\n",
        "    grads = tape.gradient(g_loss_value, generator.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
        "    \n",
        "    # Check intermediate results\n",
        "    \n",
        "    if step % 200 == 0:\n",
        "        print(\"[Step %2d] D Loss: %.4f; G Loss: %.4f\" % (\n",
        "            step, d_loss_value.numpy(), g_loss_value.numpy()))\n",
        "        noise = np.random.normal(0, 1, (8, INPUT_DIM)).astype(np.float32)\n",
        "        syntetic_images = generator.predict(noise)\n",
        "        plot_digits(syntetic_images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exdhThA6-n8w"
      },
      "source": [
        "noise = np.random.normal(0, 1, (32, INPUT_DIM)).astype(np.float32)\n",
        "syntetic_images = generator.predict(noise)\n",
        "plot_digits(syntetic_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjxJSpQFIC2e"
      },
      "source": [
        "noise_1 = np.random.normal(0, 1, (INPUT_DIM)).astype(np.float32)\n",
        "noise_2 = np.random.normal(0, 1, (INPUT_DIM)).astype(np.float32)\n",
        "noise = np.linspace(noise_1, noise_2, 32)\n",
        "syntetic_images = generator.predict(noise)\n",
        "plot_digits(syntetic_images)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}